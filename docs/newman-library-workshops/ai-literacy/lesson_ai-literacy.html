<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>AI Literacy Workshop</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a21a96fda2a35a6034e035564d362780.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="custom.css">
</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI Literacy Workshop</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a id="top"></a></p>
<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#ai-literacy-workshop">AI Literacy Workshop</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#ai-at-baruch-college">AI at Baruch College</a></li>
<li><a href="#generative-ai">Generative AI</a></li>
<li><a href="#large-language-models">Large Language Models (LLM)</a></li>
<li><a href="#retrieval-augmented-generation">Retrieval Augmented Generation (RAG)</a></li>
<li><a href="#ai-search">AI Search</a></li>
<li><a href="#considerations-and-questions-about-using-ai">Considerations and Questions about Using AI</a></li>
<li><a href="#glossary">Glossary</a></li>
</ul></li>
</ul>
<hr>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>Artificial Intelligence is undoubtedly reshaping how we access, evaluate, and create information. From <a href="#chatbot">chatbots</a> to research assistants, AI tools are becoming part of everyday academic life. Yet one of the challenges of AI is that the term itself is broad.</p>
<p>In its white paper <a href="https://cdn.openai.com/business-guides-and-resources/identifying-and-scaling-ai-use-cases.pdf">Identifying and Scaling AI Use Cases (2025)</a>, OpenAI - the company behind the foundational GPT models - proposes six “primitive” use cases: content creation, research, coding, data analysis, ideation/strategy, and automation.</p>
<p><img src="./images/overview/overview.png" alt="chat-gpt-image" class="img-sm"></p>
<p>This framework provides a practical way to think about AI’s impact on higher education by focusing on the activities that shape scholarship and learning. In particular, the categories of content creation and research speak directly to the core work of colleges and libraries, where knowledge is collected and interpreted.</p>
<p>Much of this discussion about AI centers on <a href="#generative-ai">generative AI</a> and the <a href="#llm">Large Language Model (LLM)</a> — the automated production of documents, essays and reports. What are we to make of machine-generated texts that mimic human authorship? What role should these documents play in the academic community?</p>
<p>At the same time, LLMs are transforming how we search for information. What new kinds of discovery become possible when search goes beyond <a href="#lexical-search">lexical search</a>? How might research change when answers can be synthesized from thousands of sources at once? What opportunities arise when relationships among documents are surfaced in seconds?</p>
<p>This teaching note will focus on AI’s influence on content creation (i.e., <a href="#generative-ai">generative AI</a>) and research at Baruch College.</p>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
<section id="ai-at-baruch-college" class="level2">
<h2 class="anchored" data-anchor-id="ai-at-baruch-college">AI at Baruch College</h2>
<section id="baruch-college-ai-guidelines-and-resources" class="level3">
<h3 class="anchored" data-anchor-id="baruch-college-ai-guidelines-and-resources">Baruch College AI Guidelines and Resources</h3>
<p>Baruch College embraces the importance of integrating AI across teaching, research, and operations with a focus on transparency and responsible use.</p>
<p>The use of AI in coursework is determined by course instructor, as outlined in <a href="https://acrobat.adobe.com/id/urn:aaid:sc:US:84bea687-f6b2-4cc3-8261-65fd479254c4?viewer%21megaVerb=group-discover">Baruch College’s Sample AI Use Policies for Course Syllabi</a>.</p>
<p>These range of approaches towards AI include:</p>
<ol type="1">
<li>Strictly Prohibited</li>
<li>Limited Use for Editing</li>
<li>Limited Use for Awareness</li>
<li>AI Awareness with Critical Evaluation<br>
</li>
<li>Guided Use with Attribution</li>
<li>Integrated Use for Application, and (7) Co-created Policies with Students.</li>
</ol>
<p>The use of AI in your coursework is contingent on the guidelines set by your course instructor; unauthorized use may be considered a violation of academic integrity.</p>
</section>
<section id="additional-links" class="level3">
<h3 class="anchored" data-anchor-id="additional-links">Additional Links</h3>
<p><a href="https://login.microsoftonline.com/6f60f0b3-5f06-4e09-9715-989dba8cc7d8/oauth2/authorize?client_id=00000003-0000-0ff1-ce00-000000000000&amp;response_mode=form_post&amp;response_type=code%20id_token&amp;resource=00000003-0000-0ff1-ce00-000000000000&amp;scope=openid&amp;nonce=D673AADBEB34EFECB7DBF0888199ABA3993BE63DB575100A-94AF7B533B68A5931B88101B3D5A91A83D4238B05AA1BBE63A664EB1301675D3&amp;redirect_uri=https%3A%2F%2Fcuny907%2Esharepoint%2Ecom%2F_forms%2Fdefault%2Easpx&amp;state=OD0w&amp;login_hint=JASON%2EAMEY41%40login%2Ecuny%2Eedu&amp;claims=%7B%22id_token%22%3A%7B%22xms_cc%22%3A%7B%22values%22%3A%5B%22CP1%22%5D%7D%7D%7D&amp;wsucxt=1&amp;cobrandid=11bd8083-87e0-41b5-bb78-0bc43c8a8e8a&amp;client-request-id=14a9c8a1-60d2-a000-3037-15c34cccb49e">Baruch Artificial Intelligence Resource Hub</a></p>
<p>Curated space for policy guidance and resources to support the ethical, informed use of AI.</p>
<p><a href="https://acrobat.adobe.com/id/urn:aaid:sc:US:502d7939-496d-4bed-aba3-0cc9ceb0976f?viewer%21megaVerb=group-discover">Academic Affairs Statement of Approach to Artificial Intelligence at Baruch College</a></p>
<p>Commits to integrating AI into teaching, research, and operations in ways that are ethical, responsible, and aligned with its core values.</p>
<p><a href="https://provost.baruch.cuny.edu/artificial-intelligence-think-tank/ai-use-guidance/">Guidelines for AI use at Baruch College</a></p>
<p>Establishes core principles for using generative AI at Baruch with Microsoft Copilot designated as the official, protected platform.</p>
<p><a href="https://acrobat.adobe.com/id/urn:aaid:sc:US:3ba11b0a-2890-4774-8d8c-4edbe80b1f11">Academic Integrity and AI</a></p>
<p>Affirms that unauthorized AI use is treated like any other integrity violation, with faculty setting clear syllabus rules and students responsible for compliance.</p>
<hr>
</section>
</section>
<section id="generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="generative-ai">Generative AI</h2>
<p>Generative AI refers to tools that can create new content - text, images, video, code, even music - based on written or spoken prompts. These systems are designed to recognize patterns in data and generate outputs that look and sound like human-created work.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/gen-ai/generative-ai-boxes.png" class="img-fluid figure-img"></p>
<figcaption>generative-ai</figcaption>
</figure>
</div>
<p>This gives students and researchers remarkable power: you can brainstorm ideas, visualize complex concepts, or rehearse how to explain them. But be cautious: just because AI can generate content does not mean that content is accurate, verified, or most importantly: appropriate to cite as an authority. Always consult your instructor or course syllabus to determine whether using generative AI is permitted in your coursework.</p>
<section id="text-to-text-tools" class="level3">
<h3 class="anchored" data-anchor-id="text-to-text-tools">text-to-text Tools</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://www.microsoft.com/en-us/microsoft-copilot">Microsoft Copilot</a></td>
<td>Microsoft Copilot is an AI assistant built into Microsoft 365. Copilot functions as a traditional AI-powered chatbot enabling it to summarize, analyze, and generate text and code. Copilot is the default generative AI tool at Baruch College because the CUNY-secure deployment ensures that data stays within the protected institutional environment and is not shared externally. While Copilot is widely available to the public, students and faculty should access the data-secure version through Microsoft Office 365 on the web or local installation.</td>
</tr>
<tr class="even">
<td><a href="https://chat.openai.com/">OpenAI ChatGPT</a></td>
<td>ChatGPT, powered by OpenAI’s GPT models, is widely regarded as the industry standard for conversational AI. Known for its versatility and fluency, it is a powerful general-purpose model that can generate, summarize, and analyze text across a wide range of domains.</td>
</tr>
<tr class="odd">
<td><a href="https://gemini.google.com/">Google Gemini</a></td>
<td>Gemini is Google’s multimodal model, designed to natively understand and work with text, images, video, and audio. It represents Google’s flagship LLM platform and integrates tightly with other Google products and services.</td>
</tr>
<tr class="even">
<td><a href="https://www.anthropic.com/claude">Anthropic Claude</a></td>
<td>Claude, developed by Anthropic, emphasizes safety and ethical alignment through an approach known as “Constitutional AI.” It is designed to reduce harmful outputs while still being a capable and versatile conversational model.</td>
</tr>
<tr class="odd">
<td><a href="https://ai.meta.com/llama/">Meta Llama 3</a></td>
<td>Llama 3 is Meta’s open-source family of Large Language Models, freely available for developers and researchers. It is widely used in academic and industry projects, offering strong performance with flexible licensing compared to commercial models.</td>
</tr>
<tr class="even">
<td><a href="https://mistral.ai/">Mistral</a></td>
<td>Mistral AI is a European startup developing powerful open-source and commercial LLMs, emphasizing efficiency and smaller models that run well on limited hardware.</td>
</tr>
</tbody>
</table>
</section>
<section id="text-to-image-tools" class="level3">
<h3 class="anchored" data-anchor-id="text-to-image-tools">text-to-image Tools</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://www.adobe.com/sensei/generative-ai/firefly.html">Adobe Firefly</a></td>
<td>Adobe’s generative AI image tool integrated into Creative Cloud (Photoshop, Illustrator). Firefly emphasizes safe, commercially licensed image generation for design workflows. Credits are available through CUNY’s access to Adobe Creative Cloud.</td>
</tr>
<tr class="even">
<td><a href="https://openai.com/dall-e">OpenAI DALL·E</a></td>
<td>One of the first widely adopted text-to-image generators, DALL·E can create realistic or stylized images from text prompts. It is integrated directly into ChatGPT.</td>
</tr>
<tr class="odd">
<td><a href="https://stability.ai/stable-diffusion">Stable Diffusion</a></td>
<td>An open-source text-to-image model, Stable Diffusion gives users and developers full control to run, fine-tune, and customize locally.</td>
</tr>
<tr class="even">
<td><a href="https://huggingface.co/docs/diffusers/index">Hugging Face Diffusers</a></td>
<td>A library of open-source diffusion models (including Stable Diffusion) hosted on Hugging Face. Provides tools for developers and researchers to integrate text-to-image generation into applications.</td>
</tr>
<tr class="odd">
<td><a href="https://www.midjourney.com/">MidJourney</a></td>
<td>Known for artistic and stylized outputs, MidJourney remains popular among artists and creative professionals.</td>
</tr>
</tbody>
</table>
</section>
<section id="text-to-video-tools" class="level3">
<h3 class="anchored" data-anchor-id="text-to-video-tools">text-to-video Tools</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://openai.com/sora">OpenAI Sora</a></td>
<td>OpenAI’s text-to-video model that creates short, high-quality clips with detailed scenes and realistic motion.</td>
</tr>
<tr class="even">
<td><a href="https://runwayml.com/">Runway Gen-2</a></td>
<td>Runway’s second-gen tool for generating and editing videos from text. Popular in media and design for its ease of use and editing features.</td>
</tr>
<tr class="odd">
<td><a href="https://pika.art/">Pika Labs</a></td>
<td>Pika offers fast, stylized text-to-video generation. Built around a Discord community and web app for quick, low-cost clips.</td>
</tr>
<tr class="even">
<td><a href="https://stability.ai/stable-video">Stable Video Diffusion</a></td>
<td>An open-source video diffusion model from Stability AI. Lets developers adapt text-to-video locally for custom needs.</td>
</tr>
<tr class="odd">
<td><a href="https://labs.google/fx/vei-vids/">Google Vids &amp; Veo</a></td>
<td>Google’s Veo powers video creation in tools like Google Vids, producing short clips with AI-generated visuals, narration, audio, and effects directly from a text script. Unlike most tools, it integrates speech and video in a single workflow, so users don’t need to generate them separately.</td>
</tr>
</tbody>
</table>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
</section>
<section id="large-language-models-llm" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-llm">Large Language Models (LLM)</h2>
<p>Artificial Intelligence is undoubtedly reshaping how we access, evaluate, and create information. From chatbots to research assistants, AI tools are increasingly part of everyday academic life. This teaching note focuses on a crucial piece of AI Literacy: understanding how large language models like ChatGPT, Gemini, and Claude work - and how they influence the way students and researchers discover, retrieve, and interact with information.</p>
<p>Although this note is focused on “AI Literacy,” the key part of understanding modern AI for information retrieval is maintaining a strong command of text-based Generative AI tools. This primarily means understanding the Large Language Model (LLM) with a focus on the inner-workings of popular chatbots powered by LLMs like OpenAI’s ChatGPT and Google’s Gemini.</p>
<p>The large language models that power popular technologies like ChatGPT and Gemini also drive many other AI tools discussed in this guide. These include specialized applications such as research assistants (e.g., Elicit) and consumer-facing Retrieval-Augmented Generation (RAG) tools (e.g., Google’s NotebookLM).</p>
<p>There are no doubt noteworthy developments in other frontiers of AI including advances in computer vision (e.g.&nbsp;self-driving cars, medical image recognition) and speech recognition (e.g.&nbsp;Amazon Alexa, Apple Siri), but in the domain of libraries and information literacy, “AI literacy” is a crucial skill that begins with a deep understanding of large language models and the popular models currently embraced by students and researchers.</p>
<section id="inside-the-black-box-understanding-llm-input-and-output" class="level3">
<h3 class="anchored" data-anchor-id="inside-the-black-box-understanding-llm-input-and-output">Inside the Black Box: Understanding LLM Input and Output</h3>
<p>Every time you use an AI chatbot like ChatGPT, you’re participating in a process that begins with your prompt and ends with a generated response. But what happens in between?</p>
<p>This process can be broken down into three steps: <strong>input</strong>, <strong>model</strong>, and <strong>output</strong>.</p>
<p><img src="./images/llms/chatbot-in-out.png" alt="chatbot-in-out" class="img-md"></p>
<hr>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="./images/llms/chatbot-in.png" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<p><strong>Input</strong>: The input is what you provide to the LLM, but it’s more than simple text limited by traditional lexical matching. Modern LLMs use Natural Language Processing (NLP) to understand the meaning and intent behind your query, going beyond simple keyword matching. To get the best results, you need to use effective prompt engineering practices. This involves carefully crafting your query with clear instructions, context, and constraints to guide the model’s output toward your desired goal. The quality of the input will influence the quality of the output.</p>
</div>
</div>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="./images/llms/chatbot-LLM.png" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<p><strong>LLM</strong>: The central component here is the LLM, which acts as the “brain” of the operation. This is where the core of the AI technology resides. The LLM’s capabilities are a direct result of its training, a process where it learns patterns, relationships, and knowledge from a massive dataset. The volume of data required is immense, often consisting of trillions of words scraped from a wide variety of sources, including websites, books, and academic papers. This training phase is computationally intensive and is what allows the model to generate coherent and contextually relevant text.</p>
</div>
</div>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="./images/llms/chatbot-out.png" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<p><strong>Output</strong>: The output is the response generated by the LLM. The quality and reliability of this output are ultimately determined by the data the model was trained on. Understanding the model’s training data is crucial for determining if you can trust the output. If the training data is biased, incomplete, or contains inaccuracies, the model may generate false or nonsensical information, a phenomenon known as a hallucination. Therefore, evaluating the output involves not only checking for factual accuracy but also considering the potential for bias and the model’s limitations.</p>
</div>
</div>
<hr>
</section>
<section id="understanding-what-llms-know-and-what-they-dont" class="level3">
<h3 class="anchored" data-anchor-id="understanding-what-llms-know-and-what-they-dont">Understanding What LLMs Know… and What They Don’t</h3>
<p>To evaluate the output of a large language model, it’s essential to understand what data the model was trained on. For this, a powerful principle applies: <strong>“the data is the model.”</strong></p>
<p>Most foundational LLMs are trained on massive collections of publicly available web content - things like Wikipedia, Reddit, open-access articles, and code repositories. However, they largely exclude the significant body of knowledge contained in pay-walled academic databases, dynamically generated web pages, and freely-accessible databases that aren’t easily crawl-able.</p>
<p><img src="./images/llms/data-is-the-model.png" alt="data-is-the-model" class="img"></p>
<p>Furthermore, a good deal of the most granular, timely, or academically significant data - local demographic statistics, scientific datasets, legal records, archival government data, and even historical real estate trends - lives in what we call the deep web. While technically available online, this information often requires interaction with forms, lives inside JavaScript-heavy interfaces, or is structured in ways that automated crawlers (and thus LLM training pipelines) struggle to access. If a search engine did not index it, an LLM likely did not train on it.</p>
<p>As a result, when you ask an LLM a question, you’re interacting with a system that has learned from a specific and limited subset of the web - not the full breadth of human knowledge. That’s why LLMs sometimes hallucinate or fail to answer questions that rely on specialized or hard-to-reach information. They simply haven’t seen it.</p>
<p>Large Language Models are trained on vast collections of publicly available data, giving them a broad foundational knowledge that often aligns well with the needs of undergraduate students and general education.</p>
<p><img src="./images/llms/llm-educational-resources.png" alt="llm-educational-resources" class="img-sm"></p>
<p>While this accessibility makes LLMs appealing as educational tools, their use in academic settings is still a topic of debate. Within the classroom, scholars and instructors continue to question how these tools will affect teaching, learning, and the development of critical thinking. As tools of research, LLMs need to be treated with caution.</p>
<p>It’s imporant to recognize the limitation of LLMs in conducting research. LLMs can be useful tools for brainstorming, summarizing, explanation and exploration - but they are not replacements for scholarly databases, specialized research tools, or the expertise of librarians. Valuable information remains in both library databases and the deep web - and navigating that terrain still requires human guidance and domain-specific resources.</p>
</section>
<section id="the-dominant-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="the-dominant-large-language-models">The Dominant Large Language Models</h3>
<p><img src="./images/llms/llm-logos.jpg" alt="llm-logos" class="img-md"></p>
<p>In today’s AI landscape, OpenAI’s ChatGPT, Google’s Gemini, and Anthropic’s Claude currently stand as the three dominant Large Language Models available for public use.</p>
<p>They are all highly capable, general-purpose models that have been trained to understand and generate human-like text, allowing them to excel at a wide range of information tasks.</p>
<p><strong>Note that LLM powered chatbots are all commercial products that operate under a business model. While commercial LLMs provide a “free” tier of usage, they also offer different tiers of service and are ultimately not free for full usage of all AI tools, features and applications.</strong></p>
<p><img src="./images/llms/copilot.jpg" alt="llm-logos" class="img-sm-sm"></p>
<p><strong>At Baruch College, Microsoft Copilot is the preferred AI chatbot.</strong> Based upon OpenAI’s GPT-4 models, Microsoft Copilot is an AI assistant built into Microsoft 365 applications like Word and Excel. Similar to popular AI-powered chatbots like ChatGPT and Google Gemini, Copilot maintains a Large Language Model (LLM) powered chat feature that enables it to summarize information and answer questions,</p>
<p>Data privacy remains a significant concern for the use of many AI tools. As such, Microsoft Copilot is the default generative AI tool at Baruch College because all data submitted stays within the secure CUNY environment and is not shared or used to train external models.</p>
</section>
<section id="professional-applications-of-llms" class="level3">
<h3 class="anchored" data-anchor-id="professional-applications-of-llms">Professional Applications of LLMs</h3>
<p>Domain-specialized applications of LLMs are increasingly emerging across professional fields like law, accounting, finance, and medicine. Tools such as Casetext CoCounsel in the legal profession or BloombergGPT in finance illustrate how the same foundational models that power general-purpose chatbots can be tailored with proprietary or highly specialized datasets to meet professional needs. These tools work precisely because they embed domain-specific knowledge directly into the model’s training or retrieval pipelines, reinforcing the principle that “the data is the model.”</p>
<p>For example, an accounting or finance LLM only performs well if it has been exposed to financial statements, tax codes, regulatory filings, and historical market data. Specialized data like this often escapes the training of general LLM models such as Gemini and ChatGPT - demonstrating that what an AI system “knows” is inseparable from the data it has been trained on, underscoring the importance of always asking what sources underpin the model’s authority.</p>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
</section>
<section id="retrieval-augmented-generation-rag" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h2>
<section id="how-rag-expands-the-capabilities-of-the-llm" class="level3">
<h3 class="anchored" data-anchor-id="how-rag-expands-the-capabilities-of-the-llm">How RAG Expands the Capabilities of the LLM</h3>
<p>While Large language models (LLMs) are undoubtedly powerful as reasoning and content creation tools, they are inherently static, meaning their knowledge is limited to the data they were initially trained on. This creates a significant problem when you need specialized, proprietary, or time-sensitive information, as a base LLM has no way of accessing this new data.</p>
<p><img src="images/RAG/unhappy-llm.png" alt="unhappy-llm" class="img-sm"></p>
<p>Retrieval-Augmented Generation (RAG) is an architectural approach that attempts to address this limitation. It enhances an LLM by giving it access to an external knowledge base, such as a database of academic papers or an up-to-date company wiki. The RAG system first retrieves relevant information from this external source and then uses that information as context to help the LLM generate a more accurate and current answer. This makes the LLM dynamic, allowing it to provide relevant responses that go beyond its original training data.</p>
<p><img src="images/RAG/happy-llm-1.png" alt="happy-llm-1" class="img"></p>
<p>While it’s important to recognize that large language models are trained on static datasets and have a fixed knowledge cutoff, some platforms - including ChatGPT - use retrieval-augmented generation to dynamically access and incorporate up-to-date information from external sources. If you receive a message indicating that an LLM like ChatGPT is consulting the web:</p>
<p><img src="images/RAG/searching-the-web-chat-gpt.jpg" alt="searching-the-web-chat-gpt" class="img-sm"></p>
<p>…it means the model is augmenting its static training data with live search results to respond to prompts that require recent or otherwise unavailable information.</p>
</section>
<section id="google-notebooklm-rag-powered-research-informed-by-your-documents" class="level3">
<h3 class="anchored" data-anchor-id="google-notebooklm-rag-powered-research-informed-by-your-documents">Google NotebookLM: RAG Powered Research Informed by Your Documents</h3>
<p>Google’s NotebookLM is an user-friendly, AI-powered tool that helps with research and note-taking by using Retrieval-Augmented Generation to generate responses grounded in your own documents. Designed to operate through a simple interface, it lets users upload a variety of content - such as PDFs, Google Docs, Slides, websites, and text - to create a structured “notebook” of personal sources. From there, NotebookLM can generate summaries, explanations, and answers based specifically on those user-provided materials.</p>
<p>At its core, NotebookLM leverages Google’s powerful Gemini LLM. But the tool’s power comes from its ability to harness Retrieval-Augmented Generation - grounding the AI’s output in external data provided by the user. When you upload documents, NotebookLM retrieves relevant passages and incorporates them directly into responses. This ensures higher accuracy and reduces the risk of hallucinations because the AI’s answers are verifiably tied to your sources.</p>
<p>Here is a notebook trained on 4 documents related to prompt engineering:</p>
<p><img src="images/RAG/notebook-llm-interface.jpg" alt="notebook-llm-interface" class="img-md"></p>
<p><br></p>
<p>Your notebook can be enhanced by a variety of source and file types: .pdf files, URLs, YouTube Videos, pasted text (etc…):</p>
<p><img src="images/RAG/notebook-lm-sources-upload.jpg" alt="notebook-lm-sources-upload" class="img-md"></p>
<p>From here, you can query your notebook with prompts like:</p>
<p><em>How do prompt engineering techniques and model configurations influence LLM output quality?</em></p>
<p><em>Name three prompting techniques.</em></p>
<p><em>How do models handle context?</em></p>
<p>After submitting questions of this nature, Google NotebookLM will give you a variety of responses, including summaries, direct answers, analyses, and insightful connections, all generated specifically from your uploaded documents. A helpful feature is that it sources its answers, showing you exactly which passages in your files the information is sourced from.</p>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
</section>
<section id="ai-search" class="level2">
<h2 class="anchored" data-anchor-id="ai-search">AI Search</h2>
<section id="ai-assisted-research" class="level3">
<h3 class="anchored" data-anchor-id="ai-assisted-research">AI Assisted Research</h3>
<p>AI tools powered by Large Language Models are reshaping research by moving beyond simple keyword matching. These AI search tools now interpret the meaning behind your questions, analyze large volumes of content, and deliver clear, summarized insights. As AI continues to advance quickly, it will likely transform how we search for, engage with, and understand information.</p>
</section>
<section id="smarter-searching-use-ai-to-expand-your-keywords-and-enhance-your-search" class="level3">
<h3 class="anchored" data-anchor-id="smarter-searching-use-ai-to-expand-your-keywords-and-enhance-your-search">Smarter Searching: Use AI to Expand Your Keywords and Enhance Your Search</h3>
<p>Traditional search, like a library catalog, finds resources by lexical matching - it directly matches your search terms with keywords in the resource’s metadata, such as its title or author.</p>
<p><img src="./images/ai-search/keyword-matching-1.png" alt="keyword-matching-1" class="img-md-sm"></p>
<p>A drawback of lexical matching is you can unknowingly miss potential resources unless you exhaustively search all relevant keywords:</p>
<p><img src="./images/ai-search/keyword-unmatching-2.png" alt="keyword-unmatching-2" class="img-md"></p>
<p>A new AI-led innovation in resource discovery involves LLMs generating a collection of keywords that cast a wider net for relevant resources. When you craft your search strategy for your research, it can be helpful to use a LLM powered chatbot to help you with keyword creation:</p>
<p><img src="./images/ai-search/open-ai-keywords.jpg" alt="chat-gpt-keywords" class="img-md"></p>
<p><img src="./images/ai-search/LLM-keywords.png" alt="LLM-keywords" class="img-sm-sm"></p>
<p>Instead of only searching for falsely accused in library research tools, you are now prepared to conduct a more robust search with wrongful conviction, false confession and DNA exoneration. This addition of semantically related keywords is important towards understanding how LLMs can help conduct a more exhaustive search of library resources.</p>
</section>
<section id="using-ai-to-build-search-strings" class="level3">
<h3 class="anchored" data-anchor-id="using-ai-to-build-search-strings">Using AI to Build Search Strings</h3>
<p>Once you’ve gathered a set of keywords, the next step is combining them into a structured search. This is another area where AI chatbots can be especially useful: they can take your list of terms and <strong>assemble them into complex Boolean search strings</strong> tailored for the tool you’re using.</p>
<p>Boolean logic (using operators like AND, OR, and NOT) underlies most research platforms, but the specific syntax often differs:</p>
<ul>
<li>Library catalogs may only recognize <strong>basic Boolean operators</strong> (AND, OR, NOT).<br>
</li>
<li>Scholarly databases (like ProQuest, JSTOR, or PubMed) typically support <strong>advanced options</strong> such as quotation marks for exact phrases, wildcards (e.g., * or ?), and proximity operators (e.g., NEAR/3).<br>
</li>
<li>General search engines like Google use their own conventions, such as the <code>site:</code> operator for restricting results to specific websites or domains.</li>
</ul>
<p>Because of these differences, it’s important to <strong>tell the chatbot which search tool you are building the string for</strong>. A search string that works in PubMed might not work in JSTOR or Google Scholar without adjustment.</p>
<p>The example shows how a chatbot can create a Google search string using the <code>site:</code> operator. This operator narrows results to particular publications. For instance:</p>
<p><img src="./images/ai-search/site-syntax-build-search-string.jpg" alt="site-syntax-build-search-string" class="img-md"></p>
<p>The above prompt should generate a search string similar to the following: <br> <br> <code>site:nytimes.com OR site:wsj.com OR site:ft.com OR site:theatlantic.com OR site:theguardian.com "artificial intelligence"</code></p>
</section>
<section id="deep-research-ai-powered-search-at-web-scale" class="level3">
<h3 class="anchored" data-anchor-id="deep-research-ai-powered-search-at-web-scale">Deep Research: AI-Powered Search at Web Scale</h3>
<p>If you were to have dozens of websites related to a topic you were researching:</p>
<p><img src="./images/ai-search/tabs-open-1.png" alt="tabs-open-1" class="img-md"></p>
<p>Or even a limitless ability to search hundreds of articles in high-quality news sites, like The New York Times:</p>
<p><img src="./images/ai-search/too-many-tabs.png" alt="too-many-tabs" class="img-md"></p>
<p><br></p>
<p><strong>How much time would it take to invidivually review each of these open tabs?</strong></p>
<p>It potentially could prove useful if you an automated assistant that could search, parse and interpret the relevance of each of these articles and web pages - all of which would be semantically related to your original search query. Additionally, what if the assistant was able to summarize and synthesize all of the information relevant to your query, and generate an exhaustive report with citations all linking back to the original sources?</p>
<p>This is the motivation behind a new research feature of the LLM-powered chatbots called “Deep Research.” Deep Research has become the accepted product term for this innovation in AI-assisted search and all the foundational LLMs - ChatGPT, Gemini, Claude - now offer this ability to comprehensively examine the web.</p>
<p>Deep Research is a tool accessible through the primary search box interface of the LLM-powered chatbots. Here is how you would access Deep Research through Google Gemini:</p>
<p><img src="./images/ai-search/gemini-deep-research.jpg" alt="gemini-deep-research" class="img-md"></p>
<p><br></p>
<p>Let’s investigate the web for resources on short term rentals (e.g Air BnB) and their impact on local neighborhoods:</p>
<!-- | Process | What You'll See |
|---|---|
| Once you structure a prompt for your Deep Research query, the tool will double check and ask specific, qualifying questions about what exactly you are searching for, and what resources the search should return. | <img src="./images/ai-search/deep-research-q-and-plan.png" alt="deep-research-q-and-plan" class="img-md"/> |
| As Deep Research scans the web and interprets the resources it returns, the tool will present a collection of sites that are being considered. Think of this as the visualization of having dozens (hundreds!) tabs open on your browser, and evaluating and synthesizing the information on each page. | <img src="./images/ai-search/deep-research-websites.png" alt="deep-research-websites"/> | -->
<div class="columns">
<div class="column" style="width:40%;">
<p>Once you structure a prompt for your Deep Research query, the tool will double check and ask specific, qualifying questions about what exactly you are searching for, and what resources the search should return.</p>
</div><div class="column" style="width:60%;">
<p><img src="./images/ai-search/deep-research-q-and-plan.png" class="img-md img-fluid"></p>
</div>
</div>
<div class="columns">
<div class="column" style="width:40%;">
<p>As Deep Research scans the web and interprets the resources it returns, the tool will present a collection of sites that are being considered. Think of this as the visualization of having dozens (hundreds!) tabs open on your browser, and evaluating and synthesizing the information on each page.</p>
</div><div class="column" style="width:60%;">
<p><img src="./images/ai-search/deep-research-websites.png" class="img-fluid"></p>
</div>
</div>
<p><br></p>
<p>Deep Research searches take consdierable time as: exhaustively examining the web, identifying sources by a wide semantic criteria, evaluating those resources and then synthesizing them into a report is a computationally demanding process. While researchers may be used to a search engine query taking a fraction of a second, Deep Research searches can take several minutes to complete.</p>
<p>Once completed, Deep Research will return a multi-page report summarizing its findings. Unlike traditional LLM chatbots however, everything in the report is cited and sourced in a bibliography that will include dozens of open web sources:</p>
<p><img src="./images/ai-search/deep-research-report.jpg" alt="deep-research-report" class="img-md"></p>
<p><br></p>
<p>Understand the LLM-assisted search, analysis and synthesis of countless web pages is computationally expensive. As such, all of the popular LLM chatbots limit how many Deep Research searches you can perform. Here is a Deep Research quota from OpenAI’s ChatGPT:</p>
<p><img src="./images/ai-search/chat-gpt-deep-research-limits.jpg" alt="chat-gpt-deep-research-limits" class="img-sm-sm"></p>
<p><br></p>
<p>While the ability to exhaustively search the web and organize your findings into a comprehensive report is undeniably a marvel of AI-assisted search, recognize some of these tool misrepresent their coverage of academic and scholarly literature. Any papers, reports or findings - purported to be scholarly or peer-reviewed - will only be literature that is found freely available on the web. Deep Research tools do not collect any documents from licensed or proprietary databases (e.g.&nbsp;JSTOR).</p>
</section>
<section id="ai-and-the-literature-review" class="level3">
<h3 class="anchored" data-anchor-id="ai-and-the-literature-review">AI and the Literature Review</h3>
<p><img src="./images/ai-search/ai-assistants-logos.jpg" alt="ai-assistants-logos" class="img-md"></p>
<p>An AI research assistant is a tool powered by artificial intelligence that supports various stages of academic research, particularly literature reviews. These tools are designed to streamline the research process by automating time-consuming activities like finding relevant papers, summarizing key findings, and synthesizing information across multiple documents. They work by processing vast amounts of academic literature and using large language models (LLMs) to understand, organize, and promote relevant documents and information.</p>
<p><strong>Most of these services are proprietary. While they often have a free tier to attract users, their full functionality is typically locked behind a paid subscription or license.</strong></p>
<p>Notable AI Research Assistants include:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://consensus.app/">Consensus</a></td>
<td>This tool focuses on extracting and synthesizing information directly from research papers to answer specific questions, helping users quickly understand a consensus viewpoint on a topic.</td>
</tr>
<tr class="even">
<td><a href="https://scite.ai/">Scite</a></td>
<td>Scite is known for its “Smart Citations,” which show how a paper has been cited by others, indicating whether it has been supported or contrasted by subsequent research.</td>
</tr>
<tr class="odd">
<td><a href="https://elicit.com/">Elicit</a></td>
<td>Elicit helps researchers automate parts of their literature review workflow. It finds papers, summarizes key takeaways, and extracts data from a set of search results.</td>
</tr>
<tr class="even">
<td><a href="https://www.researchrabbit.ai/">ResearchRabbit</a></td>
<td>This tool is more of a “citation-based” discovery engine. It helps you visualize a network of research papers related to your initial input, finding connected authors, journals, and topics.</td>
</tr>
</tbody>
</table>
<p><strong>AI research assistants are constrained by the academic literature they can access. AI research assistants cannot access articles behind a paywall or a database subscription. This means they cannot include research from many high-impact, subscription-based journals unless the specific article has been made open-access.</strong></p>
</section>
<section id="ai-powered-search-engines" class="level3">
<h3 class="anchored" data-anchor-id="ai-powered-search-engines">AI-Powered Search Engines</h3>
<p><img src="./images/ai-search/ai-search-assistants.jpg" alt="ai-search-assistants" class="img-sm"></p>
<p>An AI search engine is a new kind of web search tool that goes beyond returning a list of links. Instead of simply matching keywords like traditional search engines, AI search uses Large Language Models (LLMs) to interpret the meaning behind your query. This allows it to perform sophisticated semantic searches - understanding context, nuance, and intent - and return direct answers that summarize and synthesize information from multiple sources.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tool</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://www.perplexity.ai/">Perplexity AI</a></td>
<td>Often considered the leader in AI Search, it provides a direct, cited answer to your query by synthesizing information from all across the web.</td>
</tr>
<tr class="even">
<td><a href="https://you.com/">You.com</a></td>
<td>Offers a customizable search experience where you can choose which sources it draws from, including apps and websites.</td>
</tr>
<tr class="odd">
<td><a href="https://exa.ai/">Exa</a></td>
<td>Specializes in providing fast, semantic search results across large datasets.</td>
</tr>
</tbody>
</table>
<p>Popular search engines like Google and Microsoft Bing are also increasingly integrating AI techniques and Large Language Models into their core search products. This integration is representing a shift from “finding information” to “getting answers,” where the search engine acts as a more intelligent assistant rather than just a directory of web pages.</p>
</section>
<section id="ai-assistants-in-academic-databases" class="level3">
<h3 class="anchored" data-anchor-id="ai-assistants-in-academic-databases">AI Assistants in Academic Databases</h3>
<p>Once again, all the major Large Language Models are trained on vast amounts of publicly available internet data. However, these foundational LLMs do not have direct access to the rich, often paywalled, content within specialized library subscription databases such as JSTOR, Web of Science, or medical journals.</p>
<p>This means the future of AI in in-depth scholarly research won’t solely rely on a single, all-encompassing LLM. Instead, we’re likely to see the rise of specialized AI assistants embedded directly within individual library databases. Imagine: the “JSTOR AI assistant” or “Elsevier AI tools” designed to operate specifically on the vast content their platforms already host.</p>
<p><img src="./images/ai-search/ai-search-in-databases.png" alt="ai-search-in-databases" class="img-sm"></p>
<p>For researchers, this will mean adapting to a new way of interacting with search tools. You might still use a general LLM for brainstorming or refining initial search queries. However, for truly comprehensive and authoritative scholarly research, the ability to effectively utilize these separate, domain-specific AI tools within each library resource will likely become a new skill in library research.</p>
<p>At present, there are handful of databases provided by the Newman Library that offer their own, independent AI search assistant, trained on documents and data offered by that source. They include:</p>
<p><strong>Factset</strong></p>
<p><strong>Statista</strong></p>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
</section>
<section id="considerations-and-questions-about-using-ai" class="level2">
<h2 class="anchored" data-anchor-id="considerations-and-questions-about-using-ai">Considerations and Questions about Using AI</h2>
<section id="ecological-concerns" class="level3">
<h3 class="anchored" data-anchor-id="ecological-concerns">Ecological Concerns</h3>
<p>The rapid development and use of artificial intelligence (AI) has raised significant ecological concerns due to its sizeable energy demands, particularly from training and running large language models (LLMs). These processes require immense computational power, leading to a substantial increase in electricity demand for data centers, which are often powered by fossil fuels.</p>
<p>In addition, the proliferation of the data centers that power AI create a new set of environmental pressures. These facilities require vast amounts of land and, crucially, water for cooling the heat-generating servers. This can strain local water supplies, especially in drought-prone regions. The combined need for energy and water creates a complex and growing environmental footprint that researchers and policymakers are still working to fully understand and address.</p>
<p>AI providers are attempting to lessen the environmental impact of their tools and are trying to reassure researchers that <a href="https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference">they’re making progress in improving efficiency</a>.</p>
</section>
<section id="copyright-concerns-and-ai" class="level3">
<h3 class="anchored" data-anchor-id="copyright-concerns-and-ai">Copyright Concerns and AI</h3>
<p>Copyright concerns are central to current debates about AI. Training an AI system often involves copying vast amounts of text, images, or other creative works, and there is no clear consensus on whether this counts as “learning” in a way similar to humans reading books, or whether it constitutes large-scale reproduction that requires permission. Some creators and publishers argue they should be compensated when their work is used for training, while others see broad access to information as a foundation for innovation.</p>
<p>For researchers and students, the key challenge is ethical as well as legal: if a model is trained on unlicensed material, should its outputs still be used in scholarly work? Courts and policymakers have yet to settle these issues, which means users should remain cautious, follow institutional guidelines, and be transparent about when and how AI tools are employed in their research or writing.</p>
</section>
<section id="learning-risk-with-using-ai" class="level3">
<h3 class="anchored" data-anchor-id="learning-risk-with-using-ai">“Learning Risk” with using AI</h3>
<p>While AI tools offer powerful new ways to find and analyze information, they also present a significant “learning risk.” By automating complex tasks, <a href="https://arxiv.org/abs/2506.08872">AI can reduce cognitive load</a> and prevent students from engaging in the deep, critical thinking necessary for learning. This convenience can also make it easier for students to engage in academic dishonesty and cheating, bypassing the fundamental work of research and synthesis. In doing so, AI tools can undermine efforts to teach core academic skills and support genuine engagement with course material.</p>
<p>For these reasons, the judicious use of AI is critical. Students who copy and paste AI-generated content may be getting a quick answer, but they are not enjoying a fulfilling educational experience. Constructive AI use requires a critical understanding of these limitations and grounded in an awareness how to be intellectually responsible with AI.</p>
</section>
<section id="google-gemini-ai-search-results" class="level3">
<h3 class="anchored" data-anchor-id="google-gemini-ai-search-results">Google Gemini AI Search Results</h3>
<p>Google’s integration of its Gemini AI system into search results (SERPs) represents a major shift in the economics of the web. Traditionally, Google has acted as a broker between users and publishers: it indexed websites, surfaced snippets, and drove traffic outward, sustaining an ecosystem where publishers earned revenue through ads, subscriptions, and visibility. Summaries generated directly in SERPs threaten this balance by reducing the need for users to click through to original sources, concentrating attention within Google’s platform and disrupting the flow of traffic that publishers depend on.</p>
<p>For librarians, this is a development of unique concern. A healthy Internet has historically relied on the diversity of publishers, archives, and institutions that make information accessible. If AI-driven search displaces that ecosystem, we risk a less robust, less sustainable information environment.</p>
<p class="top-link">
<a href="#top">^Back to Top</a>
</p>
<hr>
</section>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p><a id="agent"></a></p>
<section id="agent" class="level3">
<h3 class="anchored" data-anchor-id="agent"><a href="https://en.wikipedia.org/wiki/Agentic_AI">Agent</a></h3>
<p>A mechanism that bundles various software and AI services together to autonomously achieve a high-level goal. At its core, a <a href="#llm">Large Language Model (LLM)</a> acts as the agent’s central intelligence, responsible for planning and interpreting user intent. However, an agent’s true capability comes from its ability to orchestrate and use external tools, which are often separate AI or software services. Many AI observers consider “agentic AI” the immediate future of the technology.</p>
<p><a id="artificial-intelligence"></a></p>
</section>
<section id="artificial-intelligence" class="level3">
<h3 class="anchored" data-anchor-id="artificial-intelligence"><a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Artificial Intelligence</a></h3>
<p>The broad field of computer science focused on creating machines that can perform tasks that typically require human intelligence, such as learning, problem-solving, perception, and <a href="#nlp">language understanding</a>.</p>
<p><a id="agi"></a></p>
</section>
<section id="artificial-general-intelligence-agi" class="level3">
<h3 class="anchored" data-anchor-id="artificial-general-intelligence-agi"><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">Artificial General Intelligence (AGI)</a></h3>
<p>A hypothetical form of AI that possesses the ability to understand, learn, and apply intelligence to solve any intellectual task that a human being can. Unlike the AI tools popular today — which are designed for a single, specific task (like a <a href="#chatbot">chatbot</a> or an image generator) — AGI would have the same kind of broad, flexible cognitive abilities as a human. AGI remains a theoretical concept, with wide-ranging technical and ethical challenges.</p>
<p><a id="chatbot"></a></p>
</section>
<section id="chatbot" class="level3">
<h3 class="anchored" data-anchor-id="chatbot"><a href="https://en.wikipedia.org/wiki/Chatbot">Chatbot</a></h3>
<p>A computer program that uses AI to conduct a conversation with a human, either through text or voice. Designed to simulate human conversation, chatbots can be used for a wide range of purposes, such as customer service, entertainment, and information retrieval.</p>
<p><a id="context-engineering"></a></p>
</section>
<section id="context-engineering" class="level3">
<h3 class="anchored" data-anchor-id="context-engineering"><a href="https://en.wikipedia.org/wiki/Prompt_engineering">Context Engineering</a></h3>
<p>A technique used in AI to provide a <a href="#llm">language model</a> with additional information, or “context,” to improve its performance and steer its output toward a desired outcome.</p>
<p><a id="context-window"></a></p>
</section>
<section id="context-window" class="level3">
<h3 class="anchored" data-anchor-id="context-window"><a href="https://en.wikipedia.org/wiki/Large_language_model#Context_window">Context Window</a></h3>
<p>The maximum amount of text (typically measured in <a href="#token">tokens</a>) that an AI model can consider at one time when generating a response. This is essentially the model’s short-term memory and a key limitation of many <a href="#llm">language models</a>.</p>
<p><a id="deep-research"></a></p>
</section>
<section id="deep-research" class="level3">
<h3 class="anchored" data-anchor-id="deep-research"><a href="https://en.wikipedia.org/wiki/ChatGPT_Deep_Research">“Deep Research”</a></h3>
<p>Product name for a recently introduced feature among popular models that acts as an <a href="#agent">agent</a> to autonomously browse the web and gather information. It generates comprehensive, cited reports on a specified topic by actively pulling in new information from the broader internet, rather than being limited to a <a href="#llm">language model’s</a> pre-existing data.</p>
<p><a id="generative-ai"></a></p>
</section>
<section id="generative-ai-1" class="level3">
<h3 class="anchored" data-anchor-id="generative-ai-1"><a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">Generative AI</a></h3>
<p>A subfield of <a href="#artificial-intelligence">artificial intelligence</a> focused on creating new, original content rather than just analyzing or classifying existing data. It uses <a href="#machine-learning">models</a> that have been <a href="#training">trained</a> on vast amounts of data to learn underlying patterns, structures, and styles. Once trained, these models can take a user <a href="#prompt-engineering">prompt</a> and generate novel outputs (text, image, sound, or video).</p>
<p><a id="gpu"></a></p>
</section>
<section id="graphical-processing-unit-gpu" class="level3">
<h3 class="anchored" data-anchor-id="graphical-processing-unit-gpu"><a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">Graphical Processing Unit (GPU)</a></h3>
<p>A specialized electronic circuit designed to rapidly perform mathematical calculations. Originally created to accelerate 3D rendering, <a href="#gpu">GPUs</a> now power tasks that require massive parallel processing, such as <a href="#machine-learning">machine learning</a> and <a href="#artificial-intelligence">artificial intelligence</a>.</p>
<p><a id="hallucination"></a></p>
</section>
<section id="hallucination" class="level3">
<h3 class="anchored" data-anchor-id="hallucination"><a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">Hallucination</a></h3>
<p>A response from an AI model that contains false, misleading, or nonsensical information, presenting it as fact. This can happen due to insufficient or biased <a href="#training-data">training data</a>, misunderstanding of context, or the model’s probabilistic nature.</p>
<p><a id="inference"></a></p>
</section>
<section id="inference" class="level3">
<h3 class="anchored" data-anchor-id="inference"><a href="https://en.wikipedia.org/wiki/Inference_(statistics)">Inference</a></h3>
<p>The process of using a <a href="#training">trained</a> AI model to make predictions or generate outputs based on new, unseen data. While <a href="#training">training</a> is resource-intensive and happens once (or periodically), inference happens in real time when you interact with an <a href="#artificial-intelligence">AI system</a>.</p>
<p><a id="lexical-search"></a></p>
</section>
<section id="lexical-search" class="level3">
<h3 class="anchored" data-anchor-id="lexical-search"><a href="https://en.wikipedia.org/wiki/Lexical_analysis">Lexical Search</a></h3>
<p>A method of searching for information that relies on the literal matching of keywords. Simple and fast, but limited, as it cannot consider meaning or context. Traditional research tools and databases rely heavily on lexical search.</p>
<p><a id="llm"></a></p>
</section>
<section id="large-language-models-llm-1" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llm-1"><a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models (LLM)</a></h3>
<p>AI models <a href="#training">trained</a> on massive amounts of text/data to understand and generate human-like language. They power many <a href="#generative-ai">generative AI</a> tools such as <a href="#chatbot">chatbots</a>, and are widely used for summarization, translation, and code generation.</p>
<p><a id="machine-learning"></a></p>
</section>
<section id="machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning"><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a></h3>
<p>A subfield of AI focused on creating algorithms that allow computers to “learn” from data without explicit programming. ML models are <a href="#training">trained</a> on large <a href="#training-data">datasets</a>. The recent “AI boom” is driven by <a href="#llm">LLMs</a> and <a href="#generative-ai">generative AI</a>, though ML has been foundational for decades.</p>
<p><a id="nlp"></a></p>
</section>
<section id="natural-language-processing-nlp" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-processing-nlp"><a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a></h3>
<p>A field of AI focused on enabling computers to understand, interpret, and generate human language.</p>
<p><a id="prompt-engineering"></a></p>
</section>
<section id="prompt-engineering" class="level3">
<h3 class="anchored" data-anchor-id="prompt-engineering"><a href="https://en.wikipedia.org/wiki/Prompt_engineering">Prompt Engineering</a></h3>
<p>The process of designing and refining the input, or “prompt,” given to a <a href="#llm">large language model (LLM)</a> to guide its output toward a specific result.</p>
<p><a id="rag"></a></p>
</section>
<section id="retrieval-augmented-generation-rag-1" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmented-generation-rag-1"><a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">Retrieval Augmented Generation (RAG)</a></h3>
<p>An AI technique that improves <a href="#llm">language models</a> by giving them access to external knowledge bases. This reduces <a href="#hallucination">hallucinations</a> and allows responses to be grounded in up-to-date, specific information.</p>
<p><a id="token"></a></p>
</section>
<section id="token" class="level3">
<h3 class="anchored" data-anchor-id="token"><a href="https://en.wikipedia.org/wiki/Tokenization_(machine_learning)">Token</a></h3>
<p>The fundamental unit of text used by an AI language model. Tokens can be words, parts of words, or punctuation. Models analyze and generate text one token at a time, and token count limits are key for <a href="#context-window">context windows</a>.</p>
<p><a id="training"></a></p>
</section>
<section id="training" class="level3">
<h3 class="anchored" data-anchor-id="training"><a href="https://en.wikipedia.org/wiki/Training_(machine_learning)">Training</a></h3>
<p>The foundational stage of building a <a href="#machine-learning">machine learning model</a>. Large amounts of <a href="#training-data">training data</a> are fed into an algorithm, adjusting its parameters for accuracy. Training requires powerful <a href="#gpu">GPUs</a> and transforms raw algorithms into usable models.</p>
<p><a id="training-data"></a></p>
</section>
<section id="training-data" class="level3">
<h3 class="anchored" data-anchor-id="training-data"><a href="https://en.wikipedia.org/wiki/Training_data">Training Data</a></h3>
<p>The vast datasets used to “teach” AI models. For <a href="#llm">LLMs</a>, this includes trillions of <a href="#token">tokens</a> scraped from the internet, digitized books, academic papers, and code. Companies often license or purchase data to build their models.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>